{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Représentations MIDI\n",
    "\n",
    "Dans la suite de ce notebook, on va essayer d'utiliser miditok pour, d'une part, créer sa propre représentation symbolique des fichiers midis, et d'autre part utiliser les représentations existantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonctionnement du worklow MidiTok :\n",
    "\n",
    "- **Preprocessing du midiFile** : le temps est downsampled à la résolution du tokenizer, les tracks sont fusionnés, les pitches en dehors du tokenizer sont enlevés, les notes et les tempos sont downsampled et les notes, le tempo et la time signature sont dédupliqués.\n",
    "\n",
    "- **Parsing des évenements globaux** : Token de tempos et de signatures temporels sont créés.\n",
    "\n",
    "- **Parsing des évenements des pistes** : Les notes, les accords, les contrôles (pédales...) et les tokens spécifiques à chaque piste sont analysés pour créer les tokens qui leur sont associés.\n",
    "\n",
    "- **Creating time tokens** : Les tokens feprésentant le temps sont créés afin de lier les jetons globaux et de suivi précédemment créés.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Représentation issue de l'article :\"Using Machine-Learning Methods for Musical Style Modeling\"\n",
    "\n",
    "Toolkit de 5 filtre simplificateurs :\n",
    "- *Filtre d'arpèges* : Aligne verticalement les notes qui ont été attaqués quasi en même temps.\n",
    "\n",
    "- *Filtre legato* : Supprime le chevauchement entre deux notes successives.\n",
    "\n",
    "- *Filtre stacato* : Ignore les silences entre les notes successives.\n",
    "\n",
    "- *Filtre de diffusion* : Aligne verticalement les diffusions de notes \n",
    "\n",
    "- *Filtre de durée* : Quantifie statistiquement les durées pour réduire l'alphabet de durée\n",
    "\n",
    "\n",
    "```lien vers la documentation```  : https://miditok.readthedocs.io/en/latest/tokenizations.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from mido import MidiFile, MidiTrack, Message\n",
    "import mido\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(midi_file, output='nested_list'):\n",
    "    \"\"\"\n",
    "    Extrait les paramètres de note d'un fichier MIDI et renvoie une matrice.\n",
    "    \n",
    "    Args:\n",
    "        midi_file: Chemin vers le fichier MIDI.\n",
    "        output: Format de sortie, 'nested_list' ou 'polars'.\n",
    "        \n",
    "    Returns:\n",
    "        Une matrice contenant les notes avec colonnes [pitch, onset, duration].\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Charger le fichier MIDI\n",
    "        mid = mido.MidiFile(midi_file)\n",
    "        notes = []\n",
    "        active_notes = {}  # clé: (track, channel, note) -> valeur: {'start_time': ..., 'velocity': ...}\n",
    "\n",
    "        # Parcourir toutes les pistes du fichier MIDI\n",
    "        for track_index, track in enumerate(mid.tracks):\n",
    "            current_time = 0\n",
    "            for msg in track:\n",
    "                current_time += msg.time\n",
    "\n",
    "                # Ignorer les messages liés au tempo ou à la signature temporelle\n",
    "                if msg.type in ['set_tempo', 'time_signature']:\n",
    "                    continue\n",
    "\n",
    "                # Note-on (activation)\n",
    "                if msg.type == 'note_on' and msg.velocity > 0:\n",
    "                    key = (track_index, msg.channel, msg.note)\n",
    "                    active_notes[key] = {\n",
    "                        'start_time': current_time,\n",
    "                        'velocity': msg.velocity\n",
    "                    }\n",
    "                # Note-off (ou note_on avec vélocité nulle)\n",
    "                elif msg.type == 'note_off' or (msg.type == 'note_on' and msg.velocity == 0):\n",
    "                    key = (track_index, msg.channel, msg.note)\n",
    "                    if key in active_notes:\n",
    "                        start_info = active_notes.pop(key)\n",
    "                        start_time = start_info['start_time']\n",
    "                        duration_ticks = current_time - start_time\n",
    "                        # On ne conserve que pitch, onset et duration\n",
    "                        notes.append({\n",
    "                            'pitch': msg.note,\n",
    "                            'onset': start_time,\n",
    "                            'duration': duration_ticks,\n",
    "                            'velocity': start_info['velocity']\n",
    "                        })\n",
    "\n",
    "        # Trier les notes par onset (temps de début)\n",
    "        notes = sorted(notes, key=lambda x: x['onset'])\n",
    "        # Construire la matrice sous forme de liste de listes\n",
    "        mnotes = [[n['pitch'], n['onset'], n['duration'], n['velocity']] for n in notes]\n",
    "\n",
    "        if output == 'nested_list':\n",
    "            return mnotes\n",
    "        elif output == 'polars':\n",
    "            return pl.DataFrame(mnotes, schema=['pitch', 'onset', 'duration', 'velocity'])\n",
    "        else:\n",
    "            raise ValueError(\"Le paramètre output doit être 'nested_list' ou 'polars'\")\n",
    "    except Exception as e:\n",
    "        print(\"Erreur lors de l'extraction:\", e)\n",
    "        return None\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Petit code snipet sur la quantization\n",
    "\"\"\"\n",
    "\n",
    "def quantize_value(value, stepSize):\n",
    "    \n",
    "    # Retourne une valeur arroondie au multiple le plus proche de stepSize\n",
    "    return np.round(value/stepSize) * stepSize\n",
    "\n",
    "def quantize_matrix(matrix, stepSize=10, quantizeOnsets=True, quantizeDurations=True):\n",
    "    \"\"\"\n",
    "    Applique plusieurs filtres sur une matrice d'événements MIDI.\n",
    "    \n",
    "    Paramètres :\n",
    "      matrix : np.ndarray de forme (N, >=2) où\n",
    "               - colonne 0 = onset\n",
    "               - colonne 1 = offset (release)\n",
    "      stepSize : unité de quantification (par défaut 10)\n",
    "      quantizeOnsets : si True, quantifie également les offsets\n",
    "      quantizeDurations : si True, quantifie la durée des notes\n",
    "      \n",
    "    Retourne :\n",
    "      La matrice quantifiée et filtrée.\n",
    "    \"\"\"\n",
    "    if isinstance(matrix,pl.DataFrame):\n",
    "      mat = matrix.clone().to_numpy()\n",
    "\n",
    "    else:\n",
    "       mat = np.array(matrix.copy())\n",
    "\n",
    "    \n",
    "\n",
    "    # On aligne le début de chaque note sur une grille de steSize = stepSize (par défaut 10ms)\n",
    "    if quantizeOnsets:\n",
    "      for i in range(len(mat)):\n",
    "        mat[i, 1] = quantize_value(mat[i, 1], stepSize=stepSize)\n",
    "\n",
    "    # On aligne les durations sur la grille\n",
    "    if quantizeDurations:\n",
    "      for i in range(len(mat)):\n",
    "        mat[i, 2] = quantize_value(mat[i, 2], stepSize=stepSize)\n",
    "\n",
    "    # On calcule l'offset après l'onset et la duration quantifié\n",
    "    offset = mat[:,1] + mat[:,2]\n",
    "\n",
    "    mat = pl.DataFrame(mat)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55869/530720673.py:58: DataOrientationWarning: Row orientation inferred during DataFrame construction. Explicitly specify the orientation by passing `orient=\"row\"` to silence this warning.\n",
      "  return pl.DataFrame(mnotes, schema=['pitch', 'onset', 'duration', 'velocity'])\n"
     ]
    }
   ],
   "source": [
    "midFile = '/home/sylogue/Documents/MuseScore4/Scores/o clair de la lune.mid'\n",
    "mid1 = extract_features(midFile, \"polars\")\n",
    "\n",
    "with open(\"midiJson.json\", \"w\") as midiJson: \n",
    "    a =mid1.write_ndjson()\n",
    "    json.dump(a, midiJson)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (2017585293.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    def NaiveImpro(symbols)\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "# Improvisation\n",
    "\n",
    "\n",
    "def NaiveImpro(symbols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
